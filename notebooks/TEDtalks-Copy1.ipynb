{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talk Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Draft: Working with BeautifulSoup\n",
    "\n",
    "A bit of experimentation led to the development of this first script for grabbing a file and getting just the AUTHOR, TITLE, DATE, LENGTH, and TEXT.\n",
    "\n",
    "```python\n",
    "import glob, re, csv\n",
    "from bs4 import BeautifulSoup as soup                                                     \n",
    "\n",
    "the_file = \"/Users/john/Code/tedtalks/test/transcript?language=en.0\"\n",
    "holding = soup(open(the_file).read(), \"lxml\")\n",
    "at = holding.find(\"title\").text\n",
    "author = at[0:at.find(':')]\n",
    "title  = at[at.find(\":\")+1 : at.find(\"|\") ]\n",
    "date = re.sub('[^a-zA-Z0-9]',' ', holding.select_one(\"span.meta__val\").text)\n",
    "length_data = holding.find_all('data', {'class' : 'talk-transcript__para__time'})\n",
    "(m, s) = ([x.get_text().strip(\"\\n\\r\") \n",
    "      for x in length_data if re.search(r\"(?s)\\d{2}:\\d{2}\", \n",
    "                                        x.get_text().strip(\"\\n\\r\"))][-1]).split(':')\n",
    "length = int(m) * 60 + int(s)\n",
    "firstpass = re.sub(r'\\([^)]*\\)', '', holding.find('div', class_ = 'talk-transcript__body').text)\n",
    "text = re.sub('[^a-zA-Z\\.\\']',' ', firstpass)\n",
    "data = [str(author), str(title)]\n",
    "# print(data)\n",
    "with open(\"./output.csv\", \"w\", newline = \"\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        for item in data:\n",
    "            writer.writerow(item)\n",
    "```\n",
    "\n",
    "After getting that to work, I imported some boilerplate that has worked for me in the past:\n",
    "\n",
    "```python\n",
    "file_list = glob.glob('/Users/john/Code/tedtalks/test/*') # produces list\n",
    "print(file_list)\n",
    "```\n",
    "\n",
    "Which produces a list of files just fine:\n",
    "\n",
    "```python\n",
    "['/Users/john/Code/tedtalks/test/transcript?language=en.0', '/Users/john/Code/tedtalks/test/transcript?language=en.1', '/Users/john/Code/tedtalks/test/transcript?language=en.2']\n",
    "```\n",
    "\n",
    "But handing that off to the initial script proved very tricky. It was clearly time to learn how to `define` functions. With more gratitude than I can express, [Padraic Cunningham][] not only developed the script below, but was also very patient in diagnosing a particular problem I encountered.\n",
    "\n",
    "The script below is available in the repo as `talks_to_csv.py`.\n",
    "\n",
    "[Padraic Cunningham]: http://chat.stackoverflow.com/users/2141635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse(the_soup):\n",
    "    # both title and author are can be parsed in separate tags.\n",
    "    author = the_soup.select_one(\"h4.h12.talk-link__speaker\").text.encode(\"utf-8\")\n",
    "    title = the_soup.select_one(\"h4.h9.m5\").text\n",
    "    # just need to strip the text from the date string, no regex needed.\n",
    "    date = the_soup.select_one(\"span.meta__val\").text.strip()      \n",
    "    # we want the last time which is the talk-transcript__para__time previous to the footer.\n",
    "    mn, sec = map(int, the_soup.select_one(\"footer.footer\").find_previous(\"data\", {\n",
    "    \"class\": \"talk-transcript__para__time\"}).text.split(\":\"))\n",
    "    length = (mn * 60 + sec)        \n",
    "    # to ignore (Applause) etc.. we can just pull from the actual text fragment checking for (\n",
    "    text = \" \".join(d.text for d in the_soup.select(\"span.talk-transcript__fragment\") if not d.text.startswith(\"(\"))        \n",
    "    # clean the text\n",
    "    text = re.sub('[^a-zA-Z\\.\\']', ' ', text)\n",
    "    return  author.strip(), title.strip(), date, length, text\n",
    "\n",
    "\n",
    "def to_csv(pth, out):\n",
    "    # open file to write to.\n",
    "    with open(out, \"w\") as out:\n",
    "        # create csv.writer. \n",
    "        wr = csv.writer(out)\n",
    "        # write our headers.\n",
    "        wr.writerow([\"author\", \"title\", \"date\", \"length\", \"text\"])\n",
    "        # get all our html files.\n",
    "        for html in os.listdir(pth):\n",
    "            with open(os.path.join(pth, html)) as f:\n",
    "                # parse the file are write the data to a row.\n",
    "                wr.writerow(parse(BeautifulSoup(f, \"lxml\")))\n",
    "                \n",
    "to_csv(\"./test\",\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix below is to remove parentheses and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def parse(soup):\n",
    "    # both title and author are can be parsed in separate tags.\n",
    "    author = soup.select_one(\"h4.h12.talk-link__speaker\").text\n",
    "    title = soup.select_one(\"h4.h9.m5\").text\n",
    "    # just need to strip the text from the date string, no regex needed.\n",
    "    date = soup.select_one(\"span.meta__val\").text.strip()\n",
    "    # we want the last time which is the talk-transcript__para__time previous to the footer.\n",
    "    mn, sec = map(int, soup.select_one(\"footer.footer\").find_previous(\"data\", {\n",
    "        \"class\": \"talk-transcript__para__time\"}).text.split(\":\"))\n",
    "    length = (mn * 60 + sec)\n",
    "    # to ignore time etc.. we can just pull from the actual text fragment and remove noise i.e (Applause).\n",
    "    text = re.sub(r'\\([^)]*\\)',\"\", \" \".join(d.text for d in soup.select(\"span.talk-transcript__fragment\")))\n",
    "    return author.strip(), title.strip(), date, length, re.sub('[^a-zA-Z\\.\\']', ' ', text)\n",
    "\n",
    "def to_csv(pth, out):\n",
    "    # open file to write to.\n",
    "    with open(out, \"w\") as out:\n",
    "        # create csv.writer.\n",
    "        wr = csv.writer(out)\n",
    "        # write our headers.\n",
    "        wr.writerow([\"author\", \"title\", \"date\", \"length\", \"text\"])\n",
    "        # get all our html files.\n",
    "        for html in os.listdir(pth):\n",
    "            with open(os.path.join(pth, html)) as f:\n",
    "                print(html)\n",
    "                # parse the file are write the data to a row.\n",
    "                wr.writerow(parse(BeautifulSoup(f, \"lxml\")))\n",
    "\n",
    "to_csv(\"./talks\",\"talks.csv\") # This is to the test directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next task is to read all the texts *qua* texts and to be able to do basic things like word frequency and topic modeling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "colnames = ['author', 'title', 'date' , 'length', 'text']\n",
    "data = pandas.read_csv('./talks-v1b.csv', names=colnames)\n",
    "talks = data.text.tolist()\n",
    "# importing all the talks here. If we want to test, we should import \n",
    "# talks from 2006 - 2015 and then train and test on 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2113"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17830"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(talks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "no_good = []\n",
    "for talk in talks: \n",
    "    A = type(talk)\n",
    "    B = type('string or something')\n",
    "    if A != B:\n",
    "        no_good.append(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185, 398, 513, 877, 1015, 1100, 2011]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in sorted(no_good, reverse=True):\n",
    "    del talks[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2106"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-checking for anything NOT a string\n",
    "i = 0\n",
    "still_no_good = []\n",
    "for talk in talks: \n",
    "    A = type(talk)\n",
    "    B = type('string or something')\n",
    "    if A != B:\n",
    "        still_no_good.append(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2106"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set(get_stop_words('en'))\n",
    "texts = [[word for word in talk.lower().split() if word not in stoplist]\n",
    "         for talk in talks]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1584"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('./talks.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(47144 unique tokens: ['abolitionists', 'pizarro', 'downstairs', 'graded', 'thrones']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)\n",
    "# To see the assignments for the tokens:\n",
    "# print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('./talks.mm', corpus) # Save corpus in Market Matrix format\n",
    "# To load this corpus: corpus = corpora.MmCorpus('./talks.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.BleiCorpus.serialize('./talks.lda-c', corpus) # To save in LDA-C format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus) # to train a portion of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus] # to transform the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
